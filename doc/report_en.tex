\documentclass{extreport}

\usepackage{amssymb, latexsym, amsmath}
\usepackage{graphicx}

\begin{document}

\section{Theory}

\subsection{NMF}

\subsubsection{Problem statement}

The formulation of non-negative matrix factorization is as follows:

For a given matrix $V$ need to find non-negative factors $W$ and $H$
such that
\[
  V \approx WH, \quad W,H \geqslant 0
\]

Now we are going consider an iterative method of approximation.
But before doing so we need to define some cost function.
It can be any measure of distance between matrices.
Let us consider the Euclidean distance:
\[
  ||A - B|| = \sqrt{\sum_{ij} (A_{ij} - B_{ij})^2}
\]

It is obvious that cost function is non-negative
and equals 0 iff $A = B$.

\textbf{Problem} For a given constant matrix $V$
and initial variable matrices $W$ and $H$
need to minimize cost function $||V - WH||$ with constraints
$W,H \geqslant 0$.

\subsubsection{Gradient descent}
Let's consider a function $F = \frac{1}{2}||V - WH||^2$.

Its explicit form is:
\[
  F = \frac{1}{2} \sum_{ij}
  \left(
    V_{ij} - (WH)_{ij}
  \right)^2
\]

Let's define $h_j$ as $j$th column of matrix $H$
and $w_i$ as $i$th row of matrix $W$.

Before introduction of multiplicative update rules
for a gradient descent let's find ordinary ones.
As it is well known gradient descent goes in direction
negative to gradient. Thus let's get formula of the gradient
for $F$ against $h_j$ and $w_i$ separetly.

\[
  F(h_j) = \frac{1}{2} \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

\[
  \frac{\partial F(h_j)}{\partial H_{sj}} =
  - \sum_i
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  W_{is} =
  -
    \left(
      \sum_i W_{is}V_{ij} - \sum_i\sum_kW_{ik}W_{is}H_{kj}
    \right)
\]

\[
  \nabla F(H) = - ( W^TV - W^TWH )
\]

\[
  F(w_i) = \frac{1}{2}\sum_j
    \left( V_{ij} - \sum_k W_{ik}H_{kj} \right)^2
\]

\[
  \frac{\partial F(w_i)}{\partial W_{ia}} =
  - \sum_j
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  H_{aj} =
  -
    \left(
      \sum_j H_{aj}V_{ij} - \sum_j \sum_k W_{ik}H_{kj}H_{aj}
    \right)
\]

\[
  \nabla F(W) = - ( VH^T - WHH^T )
\]

Then common gradient descent update rules will be:
\begin{align*}
  H_{\alpha\mu} \leftarrow H_{\alpha\mu} +
  \eta_{\alpha\mu} \left[
     (W^TV)_{\alpha\mu} - (W^TWH)_{\alpha\mu}
  \right] \\
  W_{i\alpha} \leftarrow W_{i\alpha} +
  \zeta_{i\alpha} \left[
    (VH^T)_{i\alpha} - (WHH^T)_{i\alpha}
  \right]
\end{align*}

Here $\eta_{\alpha\mu}$ and $\zeta_{i\alpha}$ must be such parameters
that guarantee the convergens of the iterative process.
Usually for a gradient descent they are taken small enough.

\subsubsection{Multiplicative update rules}

\textbf{Theorem} \textit{ The cost function $F$ in not increasing under
the update
\[
  H_{\alpha\mu} \leftarrow H_{\alpha\mu}
  \frac{(W^TV)_{\alpha\mu}}{(W^TWH)_{\alpha\mu}} \quad
  W_{i\alpha} \leftarrow W_{i\alpha}
  \frac{(VH^T)_{i\alpha}}{(WHH^T)_{i\alpha}}
\]
The cost function $F$ is invariant under these update rules if and only if
$W$ and $H$ are at a staionary point of $F$.
}

These multiplicative update rules can be achieved from common gradient
ones by settings $\eta_{\alpha\mu}$ and $\zeta_{i\alpha}$ to
the following values:
\[
  \eta_{\alpha\mu} =
  \frac {H_{\alpha\mu}}
        {(W^TWH)_{\alpha\mu}} \quad
  \zeta_{i\alpha} =
  \frac {W_{i\alpha}}
        {(WHH^T)_{i\alpha}}
\]

But as these values are not guaranteed to be small it is important
to provide proof of convergens.

At first auxiliary function definition as well as two lemmas
are introduced. Later it will be used
for a theorem proof.

\textbf{Definition} $G(h,h^t)$ is an auxiliary function for $F(h)$ if the
conditions
\[
  G(h,h') \geqslant F(h), \quad G(h,h) = F(h)
\]
are satisfied.

\textbf{Lemma 1} if $G$ is an auxiliary function,
then $F$ is nonincreasing under the update
\[
  h^{t+1} = \text{arg} \, \underset{h}{\text{min}} \; G(h,h^t)
\]

\textbf{Proof}
$F(h^{t+1}) \leqslant G(h^{t+1},h^t) \leqslant G(h^t,h^t) = F(h^t)$
$\blacksquare$

\textbf{Lemma 2} if $K(h^t)$ is the diagonal matrix
\[
  K_{ab}(h) =
  \delta_{ab}
  (\underbrace{W^T W h^t}
    _{\substack{\text{this is a}\\ \text{vector}}}
  )_a / h_a^t
\]
then
\begin{equation}\label{E:auxghht}
  G(h,h^t) = F(h^t) + (h - h^t)^T \nabla F(h^t)
    + \frac{1}{2} (h - h^t)^T K(h^t) (h - h^t)
\end{equation}
is an auxiliary function for
\[
  F(h_j) = \frac{1}{2} \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

\textbf{Proof}

The constraint $G(h,h) = F(h)$ are satisfied. Then we need only
to show that $G(h,h^t) \geqslant F(h)$.

As partial derivitives of $F$ with an order higher than 2 equal
0 then it's taylor series are:
\begin{equation}\label{E:f_taylor}
  F(h) = F(h^t) + (h - h^t)^T \nabla F(h^t)
    + \frac{1}{2} (h - h^t)^T (W^TW) (h - h^t)
\end{equation}

After comparing this equation with eq.\eqref{E:auxghht} we find that
$G(h,h^t) \geqslant F(h)$ is equivalent to
\[
  \frac{1}{2} (h - h^t)^T \left[K(h^t) - W^TW\right] (h - h^t) \geqslant 0
\]

As you might see it means positive semidefiniteness of $K(h^t) - W^TW$.
To prove that let's introduce matrix $M$
\begin{equation}
  M_{ab} = h_a^t (K(h^t) - W^T W)_{ab} h_b^t
\end{equation}
which is just a rescaling of the components of $K(h^t) - W^TW$.

\textbf{Naive example 1}
\[
  E =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}, \;
  h = \begin{bmatrix} -2 \\ 3 \end{bmatrix};
\]

\begin{align*}
  M_{11} = h_1 E_{11} h_1 = 4; \\
  M_{12} = h_1 E_{12} h_2 = 0; \\
  M_{21} = h_2 E_{21} h_1 = 0; \\
  M_{22} = h_2 E_{22} h_2 = 9; \\
\end{align*}

if $ S = \text{diag}\{h_j\}$ and $S^T A S \rightarrow B$
then $B$ is a rescaled quadratic form $A$.

$\square$

Then $K(h^t) - W^TW$ is positive semidefinite if and only if $M$ is.
And it is
\begin{multline}
  v^t M v =
  \sum_{a,b}v_a h_a^t (\delta_{ab}(W^TWh^t)_a / h_a^t
  -(W^TW)_{ab})h_b^t v_b \\
  =
  \left[
    \begin{bmatrix}1 & 1 \\ 2 & 0 \\ \end{bmatrix} \times
    \begin{bmatrix}1 & 2 \\ 1 & 0 \\ \end{bmatrix} \times
    \begin{bmatrix} -2 \\ 3 \end{bmatrix}
    =
      \begin{bmatrix}2 & 2 \\ 2 & 2 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix} =
      \begin{bmatrix} 2 \\ 2 \end{bmatrix}
  \right] \\
  =
  \left[
    \delta((W^TWh^t)_a / h_a^t) =
    \frac{1}{h_a^t} \sum_k (W^TW)_{ak} h_k^t
  \right] \\
  =
  \sum_a v_a^2 {h_a^t}^2
    \left(\sum_k(W^TW)_{ak}h_k^t\right) /h_a^t -
    \sum_{a,b} v_ah_a^t(W^TW)_{ab}h_b^tv_b \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a^2 - v_av_b) \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(
    \frac{1}{2}v_a^2 + \frac{1}{2}v_b^2 - v_av_b) \\
  = \frac{1}{2} \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a - v_a)^2 \geqslant 0 \\
\end{multline}

\textbf{Naive example 2}

\[
  A \geqslant 0 \quad \text{and} \quad
  B_{ab} = h_a^tA_{ab}h^t
  \Rightarrow B \geqslant 0
\]
\[
  v =
  \begin{bmatrix}
    1 \\ 1 \\ \vdots \\ 1
  \end{bmatrix} \Rightarrow
  v^TBv = \sum_{a,b} h_a^tA_{ab}h_b^t \geqslant 0
\]
\[
  \Rightarrow \forall v \quad
  \sum_{a,b}\left(h_a^tA_{ab}h_b^t\right) (v_a - v_b)^2 \geqslant 0
\]

$\square$

$\blacksquare$

\textbf{Proof of Theorem}

$\blacksquare$

\section{Notes}

\subsection{NMF}

\[
  V =
  \begin{bmatrix}
    1 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 \\
  \end{bmatrix}
  \approx WH =
  \begin{bmatrix}
    & & \\
    & & \\
    & & \\
    & & \\
  \end{bmatrix}
  \times
  \begin{bmatrix}
    & & & & \\
    & & & & \\
  \end{bmatrix}
\]

\[
  W =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
    0 & 0 \\
    0 & 0 \\
  \end{bmatrix} \quad
  H =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
  \end{bmatrix} \quad
  V' =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
  \end{bmatrix}
\]

\[
  N = \sum_{ij}
  \left(
    V_{ij} - (WH)_{ij}
  \right)^2
  = 7 \geqslant 0
\]

\[
  N(h_j) = \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

where $h_j$ is $j$th column of matrix $H$

\[
  \frac{\partial N(h_j)}{\partial H_{sj}} =
  -2 \sum_i
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  W_{is} =
  -2
    \left(
      \sum_i W_{is}V_{ij} - \sum_i\sum_kW_{ik}W_{is}H_{kj}
    \right)
\]

\[
  \nabla N(H) = -2 ( W^TV - W^TWH )
\]

\[
  N(w_i) = \sum_j \left( V_{ij} - \sum_k W_{ik}H_{kj} \right)^2
\]

where $w_i$ is $i$th row of matrix $W$

\[
  \frac{\partial N(w_i)}{\partial W_{ia}} =
  -2 \sum_j
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  H_{aj} =
  -2
    \left(
      \sum_j H_{aj}V_{ij} - \sum_j \sum_k W_{ik}H_{kj}H_{aj}
    \right)
\]

\[
  \nabla N(W) = -2 ( VH^T - WHH^T )
\]

\subsection{Auxiliary function and an update rule}

Let's $G(h, h^t)$ is an auxiliary function for $F(h)$ iff
\[
  G(h,h^t) \geqslant F(h) \quad \text{and}
  G(h,h^t) = F(h) \iff h = h^t
\]

\textbf{Lemma} 1.

$F(h^t)$ is not increasing under an update rule:
\[
  h^{t+1} = \text{arg} \, \underset{h}{\text{min}} \; G(h,h^t)
\]

\[
  \rhd \quad
    F(h^{t+1}) \leqslant G(h^{t+1},h^t) \leqslant G(h^t,h^t) = F(h^t) \quad
  \lhd
\]

Let's $K_{ab}(h) =
  \delta_{ab}
  (\underbrace{W^T W h}_{\substack{\text{this is a}\\ \text{vector}}})_a
  / h_a$.

Then

\textbf{Lemma} 2.
$G(h,h^t) = F(h^t) + (h - h^t)^T \nabla F(h^t)
  + \frac{1}{2} (h - h^t)^T K(h^t) (h - h^t)$
is an auxiliary function for $F(h)$.

$\rhd$

$$F(h) \approx F(h^t) + (h - h^t)^T \nabla F(h^t)
  + \frac{1}{2} (h - h^t)^T (W^TW) (h - h^t)$$

$$G(h,h^t) \geqslant F(h) \iff
\frac{1}{2} (h - h^t)^T \left[K(h^t) - W^TW\right] (h - h^t)
\geqslant O(||h - h^t||^2)$$

It is equivalent to positive semidefiniteness of a quadratic form
$$(K(h^t) - W^TW)$$

Let's $M_{ab} = h_a^t (K(h^t) - W^T W)_{ab} h_b^t$.
It is just rescaling of the quadratic form components.

\[
  E =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}, \;
  h = \begin{bmatrix} -2 \\ 3 \end{bmatrix};
\]

\begin{align*}
  M_{11} = h_1 E_{11} h_1 = 4; \\
  M_{12} = h_1 E_{12} h_2 = 0; \\
  M_{21} = h_2 E_{21} h_1 = 0; \\
  M_{22} = h_2 E_{22} h_2 = 9; \\
\end{align*}
$$ S = \text{diag}\{h_j\}; \quad S^T A S \rightarrow B$$
where $B$ is a rescaled quadratic form $A$.

So $K(h^t) - W^TW \geqslant 0 \iff M \geqslant 0$.

\begin{multline*}
  v^t M v =
  \sum_{a,b}v_a h_a^t (\delta_{ab}(W^TWh^t)_a / h_a^t
  -(W^TW)_{ab})h_b^t v_b \\
  =
  \left[
    \delta_{ab}
    \left(
      \begin{bmatrix}1 & 1 \\ 2 & 0 \\ \end{bmatrix} \times
      \begin{bmatrix}1 & 2 \\ 1 & 0 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix}
    \right) =
      \begin{bmatrix}2 & 2 \\ 2 & 2 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix} =
      \begin{bmatrix} 2 \\ 2 \end{bmatrix}
  \right] \\
  =
  \left[
    \delta((W^TWh^t)_a / h_a^t) =
    \frac{1}{h_a^t} \sum_k (W^TW)_{ak} h_k^t
  \right] \\
  =
  \sum_a v_a^2 {h_a^t}^2 (\sum_k(W^TW)_{ak}h_k^t)/h_a^t -
    \sum_{a,b} v_ah_a^t(W^TW)_{ab}h_b^tv_b \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a^2 - v_av_b) \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(
    \frac{1}{2}v_a^2 + \frac{1}{2}v_b^2 - v_av_b) \\
  = \frac{1}{2} \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a - v_a)^2 \geqslant 0 \\
\end{multline*}

\section{Simple signal}

\[
  s(t) = g(\alpha t) sin(\gamma t) + g(\beta t) sin(\delta t)
\]

where $g(\cdot)$ is a gate function with a period $2\pi$.

\[
  g(t) =
  \begin{cases}
    1, & 0 \leqslant \left( t + \frac{\pi}{2} \right)
      \; \text{mod} \; \pi \leqslant \pi \\
    0, & otherwise
  \end{cases}
\]

$$t = linspace(0, 0.3, round(0.3 \cdot 44100))$$

$$\gamma = 440 \cdot 2 \pi$$

$$\alpha = 2 \pi \cdot \frac{1}{0.3} \cdot 3$$

$$\delta = \textbf{E4} \cdot 2 \pi$$

$$\beta = \frac{2 \pi}{0.3} \cdot 2$$

$$\textbf{E4} = 440 \cdot 2^{-\frac{5}{12}}$$

\begin{figure}
  \includegraphics*[clip, scale=.6]{build/spectrum.png}
  \caption{Matrix~$V$ of a resulting signal.
  It depicts spectrogram over a time.}
\end{figure}

Each cell of $V$ --- $v_{ij}$ shows amplitude
of a signal component with a frequency $i$
at a time moment $j$.

In practice $j$ reflects time window of a width
let's say 2048 or 4096 $= L$ samples.

$i$ varies between $1, \dots, \left\lfloor \frac{L}{2}+1 \right\rfloor$.
$j$ varies between $1, \dots, \left\lceil \frac{length(s)}{L} \right\rceil$.

$$s = \text{simple\_signal\_1}$$

$$V = \text{getspectrum}(s,4096 = L)$$

$$V1 = V(1:100,:)$$

It is part $V$ with frequencies from 1 to 100 (measure is unknown,
but looks~like $\approx$ 10 Hz).

$$\text{mesh} = (V1)$$

$$FV = V1 \geqslant 1.4e+3$$

$$V2 = V1 .*  V$$

(this is selection from $V$ by amplitude threshold in $1.4 \cdot 10^3$)

$$\text{mesh}(V2)$$

$$\text{image}(FV); \text{colormap}([1, 1, 1; 0, 0, 0])$$

\end{document}
