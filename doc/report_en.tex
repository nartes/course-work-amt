\documentclass[oneside, final, 14pt]{extarticle}

\usepackage[paper=a4paper, left=3cm,
            top=2cm, bottom=2cm, right=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{aeguill}

\usepackage[main=english, russianb]{babel}

\usepackage[T1]{fontenc}

\usepackage{amssymb, latexsym, amsmath}
\usepackage{graphicx}

\usepackage{indentfirst}

\begin{document}

\begin{titlepage}

\centerline{\bf MINISTRY OF EDUCATION REPUBLIC OF BELARUS}
\bigskip
\bigskip
\centerline{\bf BELARUSIAN STATE UNIVERSITY}
\bigskip
\bigskip
\centerline{\bf FACULTY OF APPLIED MATHEMATICS AND INFORMATICS}
\bigskip
\bigskip
\centerline{\bf Department of discrete mathematics and algorithmics}
\vfill
\vfill
\vfill
\centerline{\bf SINIAK SIARHEI}
\bigskip
\bigskip
\centerline{\large \bf AUDIO RECOGNITION}
\vfill
\begin{centering}
  {
  Course work \\
  3rd year student, 3rd group, speciality
  \guillemotleft informatics\guillemotright \\}
\end{centering}
\vfill
\vfill
\hfill
\begin{minipage}{0.35\textwidth}
  {\bf Course work supervisor \\
  {\small{\it Buslavski Alexander}}}
\end{minipage}
\vfill
\vfill
\centerline{\large \bf Minsk 2016}

\restoregeometry

\end{titlepage}

\setcounter{page}{2}

\tableofcontents

\cleardoublepage

\section{Introduction}

\section{Theory}

\subsection{DSP}

DSP stands for digital signal processing. I was relying alot
onto this domain during my reasearches. Here i am going to
revise the material that was important to understanding.

Thus let's we have a discrete signal with a period $N$.

It was obtained by sampling a continuous signal
with a sampling rate 44100 per second.
Then this large signal is cut off in segments of a length 4096.

The very this segmentis source periodical signal.
Now it is a subject to \textbf{Discrete-Time Fourier Series} analysis.

Initial signal is $s(t)$, where $t$ is measured in second. Then its
sampling provides discrete signal $s[n]$ with a smpling rate 44100.
\[
  s[n] = s(nT), \quad T = 1 / 44100
\]

Now let's take a segment of length 4096, i.e. the length
of time-window, and define it as $x[n]$ ($|x[n]| = N$).

The goal is to break this chunk up to complex exponential signals
in order to analyse its power distribution among frequencies of that
harmonically related complex exponentials.

The following formula
\[
  c_k = \frac{1}{N} \sum_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N}kn}
\]
provides coefficients of \textbf{Discrete-Time Fourier Series}.

Thus
\[
  x[n] = \sum_{k=0}^{N-1} c_k e^{j\frac{2\pi}{N}kn}.
\]
The last two equations are analysis and synthesis of Fourier transform.

Let's pint out a property of Fourier Series coefficients $c_k$.
It arises due to signal $x[n]$ is real.

In general $c_k$ are complex numbers, but here
\[
  c_{-k} = c_k^* = |c_k| e^{-j\angle c_k} \text{, hence}
\]

\[
  |c_k| = |c_{-k}|, \quad \angle (c_{-k}) = -\angle c_k
\]

which means that the magnitude spectrum has even symmetry
and the phase spectrum has odd symmetry. As a result
only half of spectrum is usefull.

\subsubsection{Notes on sampling}

In the most common form of sampling known as periodic or uniform sampling,
a sequence of samples $x[n]$ is obtained from a continuous-time signal
$x_c(t)$ by taking values at equally spaced points in time.
Periodic sampling is defined by the relation
\[
  x[n] \triangleq x_c(t) \left. \right|_{t=nT} = x_c(nT), \quad
  - \infty < n < \infty
\]
where $T$, the fixed time interval between samples
is known as the sampling period. The reciprocal of the sampling
period, $F_s = 1 / T$, is called sampling frequency
(when expressed in cycles per second or Hz)
or sampling rate (when expressed in samples per second).

We now ask: are the samples $x[n]$ sufficient to describe uniquelly
the original continuous-time signal and, if so, how can $x_c(t)$
be reconstructed from $x[n]$.

Tne answer to this questions lies in the frequency domain, that is,
in the relation between the spectra of $x_c(t)$ and $x[n]$.

\subsubsection{The effect of undersampling: aliasing}

According to the sampling theorem, a continuous-time signal
$x_c(t)$ with frequencies no higher than $F_H$ can be reconstructed
exactly from its samples $x[n]=x_c(nT)$, if the samples are taken
at a rate $F_s = 1/T$ that is greater than the Nyqueist rate $2F_H$.
The spectrum of $x[n]$ is obtained by scaling spectrum of $x_c(t)$
by $F_s$ and putting copies at all integer multiplies of $F_s$.

If these copies do not overlap we can reconstruct perfectly the original
signal by taking the inverse Fourier transform of the copy
centered at zero frequency. If the spectral copies overlap, the spectrum
of $x_c(t)$ is no longer recoverable from the spectrum of $x[n]$.
In this case, the reconstructed signal $x_r(t)$ is related
to the original signal $x_c(t)$ through aliasing distortion.

\subsubsection{Sampling theorem}

Sampling thereom: \textit{Let $x_c(t)$ be a continuous-time bandlimited
signal with Fourier transform}
\[
	X_c(j\Omega) = 0 \text{\quad for \quad} |\Omega| > \Omega_H
\]
\textit{Then $x_c(t)$ can uniquely determined by its samples
$x[n]=x_c(nT)$, where $n = 0,\pm1,\pm2, \dots$, if the sampling
frequency $\Omega_s$ satisfies the condition}
\[
  \Omega_s = \frac{2\pi}{T} \geqslant 2 \Omega_H.
\]
In terms of the frequency $F = \Omega / 2\pi$ in Hz, the
conditions of the sampling theorem are:
\[
  X_c(j2\pi F) = 0 \text{\quad for \quad} F > F_H \text{ \quad and \quad}
  F_s = 1 / T \geqslant 2 F_H.
\]

\subsubsection{Terminology in sampling operation}
The highest frequency $F_H$, in Hz, present in a bandlimited signal
$x_c(t)$ is called the Nyquist frequency. The minimum sampling frequency
required to avoid overlapping bands is $2F_H$, which is called Nyquist rate.
The actual highest frequency that the sample signal $x[n]$ contains
is $F_s/2$, in Hz, and is termed as the folding frequency.

\subsubsection{Fourier transform}

$$s[n], n = 1 \dots N \quad |s[n]| = N \quad T = 44100$$
where T is sampling rate, and $F_s = 1 / T$ - fundemental frequency.

$S[k]$ -- transformed signal $s[n]$.

Let's $W_N^k = e^{j\frac{2\pi}{N}kn}, k = \overline{0,N-1}$.

$e^{i\phi} = cos\phi + i sin\phi$

System of $\left\{ W_N^k\right\}_{k=\overline{0,N-1}}$ is orthogonal and
element has it's norm equals to $N$.

Let's define $S[k] = c_k; s[n] = \sum_{k=0}^{N-1}c_kW_N^k$

$c_k = \frac{1}{N} \sum_0^{N-1} s[n] W_N^k$

$<W_N^k,W_N^m> = \sum_{n=0}^{N-1}W_N^kW_N^m$

$\sum_n e^{j\frac{2\pi}{N}(k-m)n} =
\begin{cases}
	0, k\not = m\\
  N, k  = m
\end{cases}$

\[
  cos(2\pi F_0 n T + \Theta) = cos(2 \pi f_0 n + \Theta)
\]

Periodicity in time, to be periodic we should have
$2 \pi f_0 N = 2 \pi k\in \mathbb{Z}$, i.e. $f_0 = \frac{k}{N}$,
i.e. normalized frequency has a period.

This
\[
  s[n] = e^{j\frac{2\pi}{N} k n}
\]
is complex exponential periodical discrete signal.

\begin{multline}
	s[n + N] = e^{ j \frac{2\pi}{N} k (n + N) } =
  e^{ j \frac{2\pi}{N} k n}
	e^{ j \frac{2\pi}{N} k N} = \\
 \left[
   e^{ j 2\pi k} = cos 2 \pi k + i \sin 2 \pi k = 1
 \right]
 = s[n]
\end{multline}

\[
  W_N^k =
  e^{ j \frac{2\pi}{N} k n}
\]

It is easy to see that $W_N^{k+N} = W_N^k$, and so this sequence
is periodical by $k$.

It is called a set of harmonically related complex exponential signals
with fundemental frequency $\frac{2 \pi}{N}$.

\[
  \sum_{n = 0}^{N-1} W_N^k {W_N^m}^* =
  \sum_{n = 0}^{N-1} e^{ j \frac{2\pi}{N} (k - m) n}
\]

The goal of Fourier analysis of signals is to break up all signals into
summations of sinusoidal components.

If we assume that $t$ is measured in seconds, the units of $F_0$
($A sin (2 \pi F_0 t + \Theta$) are cycles per second or Hertz (Hz).

Using Euler's identity we can express every sinusoidal signal in terms
of two complex exponentials with the same frequency:
\[
  \Omega_0 = 2 \pi F_0 \quad
  A cos(\Omega_0 t + \Theta) =
	\frac{A}{2} e^{j\Theta} e^{j\Omega_0 t} +
  \frac{A}{2} e^{-j\Theta} e^{-j\Omega_0 t}
\]

$2 \pi F_0 = \Omega$ reflects how much radians per second.

\begin{center}
  $sin(t)$ \\
  $sin(2 \pi t)$
	frequency narrrowed in $2 \pi$ times \\
  $sin(F_0 2 \pi t)$
  frequency narowed $F_0$ times
\end{center}
Thus $F_0$ is cycles of sin per second, i.e. Hertz.

System $H$ is linear iff for every real or complex constant $a_1, a_2$
and every input signal $x_1[n]$ and $x_2[n]$
\[
  H(a_1x_1[n] + a_2x_2[n]) =
  a_1 H(x_1[n]) + a_2 H(x_2[n])
\]
for all values $n$.

A system is called time-invariant or fixed iff
\[
  y[n] = H(x[n]) \Rightarrow y[n - n_0] = H(x[n - n_0])
\]
for every input signal $x$ and time shift $n_0$. That is, a time shift
in the input results in a corresponding time shift in the output.

The following equation shows with a help of convolution integral response
of the LTI system to the input $x(t) = e^{j\Omega t}$
\begin{align*}
  y(t) = \int_{-\infty}^{\infty} h(\tau)x(t - \tau) d \tau =
	\int_{-\infty}^{\infty} h(\tau) e^{j\Omega(t - \tau)} d \tau =
	\left(
    \int_{-\infty}^{\infty} h(\tau) e^{j\Omega\tau} d \tau
  \right)
	e^{j\Omega t}
\end{align*}
Note that if the integral exists its value is a complex constant that depends
on input frequency.
Thus $y(t) = H(j\Omega)e^{j\Omega t}$.

This implies that the complex exponentials are eigenfunctions
of continuous-time LTI systems.
For a specific value of $\Omega$, the constant $H(j\Omega)$ is an eigenvalue
associated with the eigenfunction $e^{j\Omega}$.
Choosing $h(t)$ so that $H(j\Omega) \approx 1$ over some range of frequencies
and $H(j\Omega) \approx 0$ over another range of frequencies,
provides the basis for the design of frequency-selective filters.

\[
  \sum_{n=<N>} s_k[n] s_k^*[n] =
  \sum_{n=<N>} e^{j \frac{2\pi}{N} kn}
		e^{-j \frac{2\pi}{N} mn} =
  \sum_{n=<N>} cos \left(\frac{2\pi}{N} kn\right)
		cos \left(\frac{2\pi}{N} mn\right)
\]

Let's provide few referencies from a DSP book on a Fourier transform.

Before sampling frequency is measured in Hertz, and after a sampling
it is measured in cycles per sample.

We synthesize a signal $x(t)$ using a linear combination of the form
\begin{equation} \label{E:423}
  x(t) = \sum_{-\infty}^{\infty} c_k e^{jk\Omega_0t}.
\end{equation}

Suppose now that \eqref{E:423} is a valid representation of a periodic
signal $x(t)$. What is the relation between coefficients $c_k$ and
the function $x(t)$? To answer this, we multiply both sides
of \eqref{E:423} by $e^{-j\Omega_0 m t}$, we change the order of
integration with summation, we integrate over a full period, and then
simplify the result using orthogonality. The answer is
\begin{equation} \label{E:424}
  c_k = \frac{1}{T} \underset{T_0}\int x(t) e^{-jk\Omega_0t} dt
\end{equation}

\eqref{E:423} and \eqref{E:424} are Synthesis and Analysis equations.

The plot of $x(t)$ as a function of time $t$ (waveform) provides
a description of the signal in the time-domain.
The plot of $c_k$ as a function of frequency $F = k F_0$ (spectrum)
constitues a description on the signal in the frequency-domain.

Since the coefficients $c_k$ are in general, complex-valued, we
can express them in polar form
\[
  c_k = |c_k| e^{j\angle c_k}
\]

The plot of $|c_k|$ is known as the magnitude spectrum
of $x(t)$, while the plot of $\angle c_k$ is known as the phase spectrum
of $x(t)$. If $c_k$ is real-valued we can use a signle plot, known
as the amplitude spectrum.

The following observations are usefull when we deal with spectra of
periodic signals:
\begin{enumerate}
\item to emphasize the frequency-domain interpretation we define
$c(kF_0) = c_k$ and plot $|c(kF_0)|$ and $\angle c(kF_0)$ as functions
of frequency $F = kF_0$.

\item the spectral lines have uniform spacing $F_0 = 1 / T_0$
determined by the fundemental period $T_0$ of $x(t)$.
The shape of $x(t)$ is specified by the values of the Fourier
coefficients $c_k$.

\item if $x(t)$ is a real function of time, we have
\[
  c_{-k} = c_k^* = |c_k| e^{-j\angle c_k} \text{, hence}
\]

\[
  |c_k| = |c_{-k}|, \quad \angle (c_{-k}) = -\angle c_k
\]
which means that the magnitude spectrum has even symmetry
and the phase spectrum has odd symmetry.

\[
  c_k = \frac{1}{N} \sum_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N}kn}
\]

\center{Discrete-Time Fourier Series}

\[
  x[n] = \sum_{k=0}^{N-1} c_k e^{j\frac{2\pi}{N}kn}.
	\leftrightarrow
  c_k = \frac{1}{N} \sum_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N}kn}
\]

\[
  c_k = \frac{1}{N} \left(\sum_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N}kn}\right)
  \Rightarrow
  \begin{aligned}
    c = \frac{1}{N} * \text{fft}(x) \\
    x = N * \text{ifft}(c)
  \end{aligned}
\]



\end{enumerate}

\subsection{NMF}

\subsubsection{Problem statement}

The formulation of non-negative matrix factorization is as follows:

For a given matrix $V$ need to find non-negative factors $W$ and $H$
such that
\[
  V \approx WH, \quad W,H \geqslant 0
\]

Now we are going consider an iterative method of approximation.
But before doing so we need to define some cost function.
It can be any measure of distance between matrices.
Let us consider the Euclidean distance:
\[
  ||A - B|| = \sqrt{\sum_{ij} (A_{ij} - B_{ij})^2}
\]

It is obvious that cost function is non-negative
and equals 0 iff $A = B$.

\textbf{Problem} For a given constant matrix $V$
and initial variable matrices $W$ and $H$
need to minimize cost function $||V - WH||$ with constraints
$W,H \geqslant 0$.

\subsubsection{Gradient descent}
Let's consider a function $F = \frac{1}{2}||V - WH||^2$.

Its explicit form is:
\[
  F = \frac{1}{2} \sum_{ij}
  \left(
    V_{ij} - (WH)_{ij}
  \right)^2
\]

Let's define $h_j$ as $j$th column of matrix $H$
and $w_i$ as $i$th row of matrix $W$.

Before introduction of multiplicative update rules
for a gradient descent let's find ordinary ones.
As it is well known gradient descent goes in direction
negative to gradient. Thus let's get formula of the gradient
for $F$ against $h_j$ and $w_i$ separetly.

\[
  F(h_j) = \frac{1}{2} \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

\[
  \frac{\partial F(h_j)}{\partial H_{sj}} =
  - \sum_i
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  W_{is} =
  -
    \left(
      \sum_i W_{is}V_{ij} - \sum_i\sum_kW_{ik}W_{is}H_{kj}
    \right)
\]

\[
  \nabla F(H) = - ( W^TV - W^TWH )
\]

\[
  F(w_i) = \frac{1}{2}\sum_j
    \left( V_{ij} - \sum_k W_{ik}H_{kj} \right)^2
\]

\[
  \frac{\partial F(w_i)}{\partial W_{ia}} =
  - \sum_j
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  H_{aj} =
  -
    \left(
      \sum_j H_{aj}V_{ij} - \sum_j \sum_k W_{ik}H_{kj}H_{aj}
    \right)
\]

\[
  \nabla F(W) = - ( VH^T - WHH^T )
\]

Then common gradient descent update rules will be:
\begin{align*}
  H_{\alpha\mu} \leftarrow H_{\alpha\mu} +
  \eta_{\alpha\mu} \left[
     (W^TV)_{\alpha\mu} - (W^TWH)_{\alpha\mu}
  \right] \\
  W_{i\alpha} \leftarrow W_{i\alpha} +
  \zeta_{i\alpha} \left[
    (VH^T)_{i\alpha} - (WHH^T)_{i\alpha}
  \right]
\end{align*}

Here $\eta_{\alpha\mu}$ and $\zeta_{i\alpha}$ must be such parameters
that guarantee the convergens of the iterative process.
Usually for a gradient descent they are taken small enough.

\subsubsection{Multiplicative update rules}

\textbf{Theorem} \textit{ The cost function $F$ in not increasing under
the update
\[
  H_{\alpha\mu} \leftarrow H_{\alpha\mu}
  \frac{(W^TV)_{\alpha\mu}}{(W^TWH)_{\alpha\mu}} \quad
  W_{i\alpha} \leftarrow W_{i\alpha}
  \frac{(VH^T)_{i\alpha}}{(WHH^T)_{i\alpha}}
\]
The cost function $F$ is invariant under these update rules if and only if
$W$ and $H$ are at a staionary point of $F$.
}

\textbf{Naive note}

Multiplicative rules are faster
then ordinary gradient descent rules.
But they have disadvantage which is expressed
in denominator, i.e. $WH$. As process approaches
$V$ which has 0, they appear in the denominator
and it results in {\it devision by zero}.

$\square$

These multiplicative update rules can be achieved from common gradient
ones by settings $\eta_{\alpha\mu}$ and $\zeta_{i\alpha}$ to
the following values:
\[
  \eta_{\alpha\mu} =
  \frac {H_{\alpha\mu}}
        {(W^TWH)_{\alpha\mu}} \quad
  \zeta_{i\alpha} =
  \frac {W_{i\alpha}}
        {(WHH^T)_{i\alpha}}
\]

But as these values are not guaranteed to be small it is important
to provide proof of convergens.

At first auxiliary function definition as well as two lemmas
are introduced. Later it will be used
for a theorem proof.

\textbf{Definition} $G(h,h^t)$ is an auxiliary function for $F(h)$ if the
conditions
\[
  G(h,h') \geqslant F(h), \quad G(h,h) = F(h)
\]
are satisfied.

\textbf{Lemma 1} if $G$ is an auxiliary function,
then $F$ is nonincreasing under the update
\[
  h^{t+1} = \text{arg} \, \underset{h}{\text{min}} \; G(h,h^t)
\]

\textbf{Proof}
$F(h^{t+1}) \leqslant G(h^{t+1},h^t) \leqslant G(h^t,h^t) = F(h^t)$
$\blacksquare$

\textbf{Lemma 2} if $K(h^t)$ is the diagonal matrix
\[
  K_{ab}(h) =
  \delta_{ab}
  (\underbrace{W^T W h^t}
    _{\substack{\text{this is a}\\ \text{vector}}}
  )_a / h_a^t
\]
then
\begin{equation}\label{E:auxghht}
  G(h,h^t) = F(h^t) + (h - h^t)^T \nabla F(h^t)
    + \frac{1}{2} (h - h^t)^T K(h^t) (h - h^t)
\end{equation}
is an auxiliary function for
\[
  F(h_j) = \frac{1}{2} \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

\textbf{Proof}

The constraint $G(h,h) = F(h)$ are satisfied. Then we need only
to show that $G(h,h^t) \geqslant F(h)$.

As partial derivitives of $F$ with an order higher than 2 equal
0 then it's taylor series are:
\begin{equation}\label{E:f_taylor}
  F(h) = F(h^t) + (h - h^t)^T \nabla F(h^t)
    + \frac{1}{2} (h - h^t)^T (W^TW) (h - h^t)
\end{equation}

After comparing this equation with eq.\eqref{E:auxghht} we find that
$G(h,h^t) \geqslant F(h)$ is equivalent to
\[
  \frac{1}{2} (h - h^t)^T \left[K(h^t) - W^TW\right] (h - h^t) \geqslant 0
\]

As you might see it means positive semidefiniteness of $K(h^t) - W^TW$.
To prove that let's introduce matrix $M$
\begin{equation}
  M_{ab} = h_a^t (K(h^t) - W^T W)_{ab} h_b^t
\end{equation}
which is just a rescaling of the components of $K(h^t) - W^TW$.

\textbf{Naive example 1}
\[
  E =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}, \;
  h = \begin{bmatrix} -2 \\ 3 \end{bmatrix};
\]

\begin{align*}
  M_{11} = h_1 E_{11} h_1 = 4; \\
  M_{12} = h_1 E_{12} h_2 = 0; \\
  M_{21} = h_2 E_{21} h_1 = 0; \\
  M_{22} = h_2 E_{22} h_2 = 9; \\
\end{align*}

if $ S = \text{diag}\{h_j\}$ and $S^T A S \rightarrow B$
then $B$ is a rescaled quadratic form $A$.

$\square$

Then $K(h^t) - W^TW$ is positive semidefinite if and only if $M$ is.
And it is
\begin{multline}
  v^t M v =
  \sum_{a,b}v_a h_a^t (\delta_{ab}(W^TWh^t)_a / h_a^t
  -(W^TW)_{ab})h_b^t v_b \\
  =
  \left[
    \begin{bmatrix}1 & 1 \\ 2 & 0 \\ \end{bmatrix} \times
    \begin{bmatrix}1 & 2 \\ 1 & 0 \\ \end{bmatrix} \times
    \begin{bmatrix} -2 \\ 3 \end{bmatrix}
    =
      \begin{bmatrix}2 & 2 \\ 2 & 2 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix} =
      \begin{bmatrix} 2 \\ 2 \end{bmatrix}
  \right] \\
  =
  \left[
    \delta((W^TWh^t)_a / h_a^t) =
    \frac{1}{h_a^t} \sum_k (W^TW)_{ak} h_k^t
  \right] \\
  =
  \sum_a v_a^2 {h_a^t}^2
    \left(\sum_k(W^TW)_{ak}h_k^t\right) /h_a^t -
    \sum_{a,b} v_ah_a^t(W^TW)_{ab}h_b^tv_b \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a^2 - v_av_b) \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(
    \frac{1}{2}v_a^2 + \frac{1}{2}v_b^2 - v_av_b) \\
  = \frac{1}{2} \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a - v_a)^2 \geqslant 0 \\
\end{multline}

\textbf{Naive example 2}

\[
  A \geqslant 0 \quad \text{and} \quad
  B_{ab} = h_a^tA_{ab}h^t
  \Rightarrow B \geqslant 0
\]
\[
  v =
  \begin{bmatrix}
    1 \\ 1 \\ \vdots \\ 1
  \end{bmatrix} \Rightarrow
  v^TBv = \sum_{a,b} h_a^tA_{ab}h_b^t \geqslant 0
\]
\[
  \Rightarrow \forall v \quad
  \sum_{a,b}\left(h_a^tA_{ab}h_b^t\right) (v_a - v_b)^2 \geqslant 0
\]

$\square$

$\blacksquare$

\textbf{Proof of Theorem}

$\blacksquare$

\section{Notes}

\subsection{NMF}

\[
  V =
  \begin{bmatrix}
    1 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 1 & 0 & 1 \\
  \end{bmatrix}
  \approx WH =
  \begin{bmatrix}
    & & \\
    & & \\
    & & \\
    & & \\
  \end{bmatrix}
  \times
  \begin{bmatrix}
    & & & & \\
    & & & & \\
  \end{bmatrix}
\]

\[
  W =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
    0 & 0 \\
    0 & 0 \\
  \end{bmatrix} \quad
  H =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
  \end{bmatrix} \quad
  V' =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
  \end{bmatrix}
\]

\[
  N = \sum_{ij}
  \left(
    V_{ij} - (WH)_{ij}
  \right)^2
  = 7 \geqslant 0
\]

\[
  N(h_j) = \sum_{i}
  \left(
    V_{ij} - \sum_{k}W_{ik}H_{kj}
  \right)^2
\]

where $h_j$ is $j$th column of matrix $H$

\[
  \frac{\partial N(h_j)}{\partial H_{sj}} =
  -2 \sum_i
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  W_{is} =
  -2
    \left(
      \sum_i W_{is}V_{ij} - \sum_i\sum_kW_{ik}W_{is}H_{kj}
    \right)
\]

\[
  \nabla N(H) = -2 ( W^TV - W^TWH )
\]

\[
  N(w_i) = \sum_j \left( V_{ij} - \sum_k W_{ik}H_{kj} \right)^2
\]

where $w_i$ is $i$th row of matrix $W$

\[
  \frac{\partial N(w_i)}{\partial W_{ia}} =
  -2 \sum_j
    \left(
      V_{ij} - \sum_k W_{ik}H_{kj}
    \right)
  H_{aj} =
  -2
    \left(
      \sum_j H_{aj}V_{ij} - \sum_j \sum_k W_{ik}H_{kj}H_{aj}
    \right)
\]

\[
  \nabla N(W) = -2 ( VH^T - WHH^T )
\]

\subsection{Auxiliary function and an update rule}

Let's $G(h, h^t)$ is an auxiliary function for $F(h)$ iff
\[
  G(h,h^t) \geqslant F(h) \quad \text{and}
  G(h,h^t) = F(h) \iff h = h^t
\]

\textbf{Lemma} 1.

$F(h^t)$ is not increasing under an update rule:
\[
  h^{t+1} = \text{arg} \, \underset{h}{\text{min}} \; G(h,h^t)
\]

\[
  \rhd \quad
    F(h^{t+1}) \leqslant G(h^{t+1},h^t) \leqslant G(h^t,h^t) = F(h^t) \quad
  \lhd
\]

Let's $K_{ab}(h) =
  \delta_{ab}
  (\underbrace{W^T W h}_{\substack{\text{this is a}\\ \text{vector}}})_a
  / h_a$.

Then

\textbf{Lemma} 2.
$G(h,h^t) = F(h^t) + (h - h^t)^T \nabla F(h^t)
  + \frac{1}{2} (h - h^t)^T K(h^t) (h - h^t)$
is an auxiliary function for $F(h)$.

$\rhd$

$$F(h) \approx F(h^t) + (h - h^t)^T \nabla F(h^t)
  + \frac{1}{2} (h - h^t)^T (W^TW) (h - h^t)$$

$$G(h,h^t) \geqslant F(h) \iff
\frac{1}{2} (h - h^t)^T \left[K(h^t) - W^TW\right] (h - h^t)
\geqslant O(||h - h^t||^2)$$

It is equivalent to positive semidefiniteness of a quadratic form
$$(K(h^t) - W^TW)$$

Let's $M_{ab} = h_a^t (K(h^t) - W^T W)_{ab} h_b^t$.
It is just rescaling of the quadratic form components.

\[
  E =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1 \\
  \end{bmatrix}, \;
  h = \begin{bmatrix} -2 \\ 3 \end{bmatrix};
\]

\begin{align*}
  M_{11} = h_1 E_{11} h_1 = 4; \\
  M_{12} = h_1 E_{12} h_2 = 0; \\
  M_{21} = h_2 E_{21} h_1 = 0; \\
  M_{22} = h_2 E_{22} h_2 = 9; \\
\end{align*}
$$ S = \text{diag}\{h_j\}; \quad S^T A S \rightarrow B$$
where $B$ is a rescaled quadratic form $A$.

So $K(h^t) - W^TW \geqslant 0 \iff M \geqslant 0$.

\begin{multline*}
  v^t M v =
  \sum_{a,b}v_a h_a^t (\delta_{ab}(W^TWh^t)_a / h_a^t
  -(W^TW)_{ab})h_b^t v_b \\
  =
  \left[
    \delta_{ab}
    \left(
      \begin{bmatrix}1 & 1 \\ 2 & 0 \\ \end{bmatrix} \times
      \begin{bmatrix}1 & 2 \\ 1 & 0 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix}
    \right) =
      \begin{bmatrix}2 & 2 \\ 2 & 2 \\ \end{bmatrix} \times
      \begin{bmatrix} -2 \\ 3 \end{bmatrix} =
      \begin{bmatrix} 2 \\ 2 \end{bmatrix}
  \right] \\
  =
  \left[
    \delta((W^TWh^t)_a / h_a^t) =
    \frac{1}{h_a^t} \sum_k (W^TW)_{ak} h_k^t
  \right] \\
  =
  \sum_a v_a^2 {h_a^t}^2 (\sum_k(W^TW)_{ak}h_k^t)/h_a^t -
    \sum_{a,b} v_ah_a^t(W^TW)_{ab}h_b^tv_b \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a^2 - v_av_b) \\
  = \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(
    \frac{1}{2}v_a^2 + \frac{1}{2}v_b^2 - v_av_b) \\
  = \frac{1}{2} \sum_{a,b}h_a^t(W^TW)_{ab}h_b^t(v_a - v_a)^2 \geqslant 0 \\
\end{multline*}

\section{Simple signal}

\[
  s(t) = g(\alpha t) sin(\gamma t) + g(\beta t) sin(\delta t)
\]

where $g(\cdot)$ is a gate function with a period $2\pi$.

\[
  g(t) =
  \begin{cases}
    1, & 0 \leqslant \left( t + \frac{\pi}{2} \right)
      \; \text{mod} \; \pi \leqslant \pi \\
    0, & otherwise
  \end{cases}
\]

$$t = linspace(0, 0.3, round(0.3 \cdot 44100))$$

$$\gamma = 440 \cdot 2 \pi$$

$$\alpha = 2 \pi \cdot \frac{1}{0.3} \cdot 3$$

$$\delta = \textbf{E4} \cdot 2 \pi$$

$$\beta = \frac{2 \pi}{0.3} \cdot 2$$

$$\textbf{E4} = 440 \cdot 2^{-\frac{5}{12}}$$

\begin{figure}
  \includegraphics*[clip, scale=.6]{build/spectrum.png}
  \caption{Matrix~$V$ of a resulting signal.
  It depicts spectrogram over a time.}
\end{figure}

Each cell of $V$ --- $v_{ij}$ shows amplitude
of a signal component with a frequency $i$
at a time moment $j$.

In practice $j$ reflects time window of a width
let's say 2048 or 4096 $= L$ samples.

$i$ varies between $1, \dots, \left\lfloor \frac{L}{2}+1 \right\rfloor$.
$j$ varies between $1, \dots, \left\lceil \frac{length(s)}{L} \right\rceil$.

$$s = \text{simple\_signal\_1}$$

$$V = \text{getspectrum}(s,4096 = L)$$

$$V1 = V(1:100,:)$$

It is part $V$ with frequencies from 1 to 100 (measure is unknown,
but looks~like $\approx$ 10 Hz).

$$\text{mesh} = (V1)$$

$$FV = V1 \geqslant 1.4e+3$$

$$V2 = V1 .*  V$$

(this is selection from $V$ by amplitude threshold in $1.4 \cdot 10^3$)

$$\text{mesh}(V2)$$

$$\text{image}(FV); \text{colormap}([1, 1, 1; 0, 0, 0])$$

\section{Misc}

\textbf{Naive note}
It, i.e. gradient descent with multiplicative rules,
works but in practice requires
upper bound for iterations. It is so
because a quite general modeling approach.
Need further investigaion.

$\square$

\section{Conclusion}

\subsection{DSP}

	I studied a little DSP. I revised Fourier transform despite of the fact
that paper's sources utilizes built-in fast fourier transform algorithm to
Octave. More about DSP. I know that signal is physical property that can varies
and thus produce function of time if it goes about analogous one. And actually
any function of time might be considered as a signal. Then I read that signal
can be continuous and discreat. Usually it depends on its nature and a system
that processes it. Because they can be transformed from one type to another.
There are two main dimensions, i.e. time and value and both can be continues or
discreate. When we are taking continues signal at a discreate time moments we
are performing sampling and thus produce sampled signal or discreat one. The
periodicity of sampling precisly dots per second is called sampling rate. Now
lets consider dimensions of a signal value. Depending on a signal nature it can
be a number or a vector. But anyway we are trying to store this number in a
computer when we are performing quantization because in general we can save to
a certain degree of precise, e.g. it can be 128 bit or 2 byte integer number,
or a double precision real number, anyway it holds finite amount of digits.
Both sampling and quantization can be traded off on demand. You can trade off
between precision or less memory amount in the simplest case.

Signals might be finite or infinite. They can be periodical or not. There is a
lot specific and important signals that are considered in DSP. They are unit
step function, impulse function. Note function and signal terms are used
interchangeably. Of course sinusoidal and cosinusoidal signals or better to say
waveforms amongh important ones.

Of course signal has its length in a fiite case. After that appears fourier
transform, that allows to unfold a signal into a sum of elementary periodical
signals, i.e. complex exponents, that are rescaled, or scaled harmonically, it
is a linear combination of this signals to be precise. Because they usually
present as parts in the signal and each of them add to total signal.

For example to take into consideration piano and guitar their recording are
different. Sound profile here refers to a fourier transform of audio signal.
The transform might also be called spectrogram. It represents the power of
elementary signals in a mixture. In a fourier transform these are coefficients
in a sum. Each elementary signal has a frequency which can expressed in a Hz
units, i.e. repeatetion per seoncd. And most important is that signal reduction
by this signals represents unfolding a signal to frequiencies it is composed
of. In a FT elementary signal frequiencies are scaled equally.

So take the same on a piano and a guitar and profile will look like:






The first figure stands for a guitar note profile and the second for a piano
one.  They both have equal frequencies of the highest peak, but piano's profile
contains extra frequencies with descending amplitudes. They are called
overtones. It is because physical nature of piano, so it makes piano sound
different, distinguishable from guitar's one. The frequencies are scaled by
integer multiplier 2,3, … comparing to main or “F0” one.

In a pair with FT stands Sampling Theorem. It stands that for a discreat signal
of a length L in a perfect reconstruction only frequencies up to L / 2 are to
revealed.

One to this fact usually it a time window has a length of say 4096 bit only
half of a spectrogram matters. For example from 1 to LowerBound(4096 / 2) Hz.
The rest will be mirror reflection.

That was enough to apply fourier transform to a live recording from a laptop
microphone. Then a resulting spectrogram was analysed manually. An interesting
fact was that despite of sinking board fan to a microphone it doesn't spoil
spectrogram and it clearly reveal three notes that were performed by voice. In
report even demonstrated the frequency of peak crresponding to a note A3 and it
does really stand closer to A3 rather than to neighbor semitones G3\# and A3\#.

But there were not presented even emperical results of this facts. So it surely
looks like a luck or intentional fooling of readers with a succefull approach.

It would be nice to provide solid statistics about this method. It can
automated by generating notes along and then performing on them global maximum
search. Then robustness may checked out under different circumstances like a
noise mixture, length rescaling or instrument replacement.

Also current report has lack of solid theory on a fourier transform. It was not
even covered with tests in source codes.


\end{document}
